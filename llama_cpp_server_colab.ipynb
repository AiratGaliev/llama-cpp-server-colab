{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AiratGaliev/llama-cpp-server-colab/blob/main/llama_cpp_server_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC4zIEuXlTqg",
    "outputId": "bea6ab6d-53cc-4306-d0df-6f685766a785"
   },
   "outputs": [],
   "source": [
    "#@title Start llama-cpp-server\n",
    "\n",
    "#@markdown If unsure about the branch, write \"main\" or leave it blank.\n",
    "%cd /content\n",
    "!apt-get -y install -qq aria2\n",
    "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | tee /etc/apt/sources.list.d/ngrok.list\n",
    "!apt update\n",
    "!apt list --upgradable\n",
    "!apt dist-upgrade\n",
    "!apt install ngrok\n",
    "!CUDACXX=/usr/local/cuda/bin/nvcc CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python[server]==0.2.39\n",
    "import json\n",
    "\n",
    "import uvicorn\n",
    "from llama_cpp.server.app import create_app\n",
    "from llama_cpp.server.settings import ConfigFileSettings, ServerSettings\n",
    "\n",
    "# Parameters\n",
    "# chat_format look here https://github.com/abetlen/llama-cpp-python/blob/main/llama_cpp/llama_chat_format.py#L613\n",
    "ngrok_auth_token = \"\" # @param {type:\"string\"}\n",
    "ngrok_domain = \"wasp-immortal-factually.ngrok-free.app\" # @param {type:\"string\"}\n",
    "n_gpu_layers = -1 # @param {type:\"integer\"}\n",
    "offload_kqv = True # @param {type:\"boolean\"}\n",
    "n_threads = 12 # @param {type:\"integer\"}\n",
    "n_batch = 1024 # @param {type:\"integer\"}\n",
    "n_ctx = 16384 # @param {type:\"integer\"}\n",
    "cache = False # @param {type:\"boolean\"}\n",
    "quant_method = \"Q6_K\" # @param [\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\"]\n",
    "# @markdown 1. Model\n",
    "model_url_1 = \"TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF\" # @param {type:\"string\"}\n",
    "model_alias_1 = \"dolphin\" # @param {type:\"string\"}\n",
    "chat_format_1 = \"chatml\" # @param [\"openchat\", \"chatml\", \"llama-2\", \"alpaca\", \"qwen\", \"vicuna\", \"oasst_llama\", \"baichuan-2\", \"baichuan\", \"openbuddy\", \"redpajama-incite\", \"snoozy\", \"phind\", \"intel\", \"open-orca\", \"mistrallite\", \"zephyr\", \"pygmalion\", \"mistral-instruct\", \"chatglm3\", \"saiga\", \"functionary\"]\n",
    "# @markdown 2. Model\n",
    "model_url_2 = \"TheBloke/openchat-3.5-0106-GGUF\" # @param {type:\"string\"}\n",
    "model_alias_2 = \"openchat\" # @param {type:\"string\"}\n",
    "chat_format_2 = \"openchat\" # @param [\"openchat\", \"chatml\", \"llama-2\", \"alpaca\", \"qwen\", \"vicuna\", \"oasst_llama\", \"baichuan-2\", \"baichuan\", \"openbuddy\", \"redpajama-incite\", \"snoozy\", \"phind\", \"intel\", \"open-orca\", \"mistrallite\", \"zephyr\", \"pygmalion\", \"mistral-instruct\", \"chatglm3\", \"saiga\", \"functionary\"]\n",
    "include_model_2 = True #@param {type:\"boolean\"}\n",
    "# @markdown 3. Model\n",
    "model_url_3 = \"TheBloke/OpenHermes-2.5-Mistral-7B-16k-GGUF\" # @param {type:\"string\"}\n",
    "model_alias_3 = \"openhermes\" # @param {type:\"string\"}\n",
    "chat_format_3 = \"chatml\" # @param [\"openchat\", \"chatml\", \"llama-2\", \"alpaca\", \"qwen\", \"vicuna\", \"oasst_llama\", \"baichuan-2\", \"baichuan\", \"openbuddy\", \"redpajama-incite\", \"snoozy\", \"phind\", \"intel\", \"open-orca\", \"mistrallite\", \"zephyr\", \"pygmalion\", \"mistral-instruct\", \"chatglm3\", \"saiga\", \"functionary\"]\n",
    "include_model_3 = True # @param {type:\"boolean\"}\n",
    "# @markdown 4. Model\n",
    "model_url_4 = \"TheBloke/xDAN-L1-Chat-RL-v1-GGUF\" # @param {type:\"string\"}\n",
    "model_alias_4 = \"xdan\" # @param {type:\"string\"}\n",
    "chat_format_4 = \"chatml\" # @param [\"openchat\", \"chatml\", \"llama-2\", \"alpaca\", \"qwen\", \"vicuna\", \"oasst_llama\", \"baichuan-2\", \"baichuan\", \"openbuddy\", \"redpajama-incite\", \"snoozy\", \"phind\", \"intel\", \"open-orca\", \"mistrallite\", \"zephyr\", \"pygmalion\", \"mistral-instruct\", \"chatglm3\", \"saiga\", \"functionary\"]\n",
    "include_model_4 = True # @param {type:\"boolean\"}\n",
    "# @markdown 5. Model\n",
    "model_url_5 = \"TheBloke/Starling-LM-7B-alpha-GGUF\" # @param {type:\"string\"}\n",
    "model_alias_5 = \"starling\" # @param {type:\"string\"}\n",
    "chat_format_5 = \"openchat\" # @param [\"openchat\", \"chatml\", \"llama-2\", \"alpaca\", \"qwen\", \"vicuna\", \"oasst_llama\", \"baichuan-2\", \"baichuan\", \"openbuddy\", \"redpajama-incite\", \"snoozy\", \"phind\", \"intel\", \"open-orca\", \"mistrallite\", \"zephyr\", \"pygmalion\", \"mistral-instruct\", \"chatglm3\", \"saiga\", \"functionary\"]\n",
    "include_model_5 = True # @param {type:\"boolean\"}\n",
    "# @markdown 6. Model\n",
    "model_url_6 = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\" # @param {type:\"string\"}\n",
    "model_alias_6 = \"capybarahermes\" # @param {type:\"string\"}\n",
    "chat_format_6 = \"chatml\" # @param [\"openchat\", \"chatml\", \"llama-2\", \"alpaca\", \"qwen\", \"vicuna\", \"oasst_llama\", \"baichuan-2\", \"baichuan\", \"openbuddy\", \"redpajama-incite\", \"snoozy\", \"phind\", \"intel\", \"open-orca\", \"mistrallite\", \"zephyr\", \"pygmalion\", \"mistral-instruct\", \"chatglm3\", \"saiga\", \"functionary\"]\n",
    "include_model_6 = True # @param {type:\"boolean\"}\n",
    "\n",
    "json_cfg = {\n",
    "    \"host\": \"0.0.0.0\",\n",
    "    \"port\": 1234,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model\": model_url_1,\n",
    "            \"model_alias\": model_alias_1,\n",
    "            \"chat_format\": chat_format_1,\n",
    "            \"include_model\": True\n",
    "        },\n",
    "        {\n",
    "            \"model\": model_url_2,\n",
    "            \"model_alias\": model_alias_2,\n",
    "            \"chat_format\": chat_format_2,\n",
    "            \"include_model\": include_model_2\n",
    "        },\n",
    "        {\n",
    "            \"model\": model_url_3,\n",
    "            \"model_alias\": model_alias_3,\n",
    "            \"chat_format\": chat_format_3,\n",
    "            \"include_model\": include_model_3\n",
    "        },\n",
    "        {\n",
    "            \"model\": model_url_4,\n",
    "            \"model_alias\": model_alias_4,\n",
    "            \"chat_format\": chat_format_4,\n",
    "            \"include_model\": include_model_4\n",
    "        },\n",
    "        {\n",
    "            \"model\": model_url_5,\n",
    "            \"model_alias\": model_alias_5,\n",
    "            \"chat_format\": chat_format_5,\n",
    "            \"include_model\": include_model_5\n",
    "        },\n",
    "        {\n",
    "            \"model\": model_url_6,\n",
    "            \"model_alias\": model_alias_6,\n",
    "            \"chat_format\": chat_format_6,\n",
    "            \"include_model\": include_model_6\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "models_dir = \"/content/models\"\n",
    "json_cfg[\"models\"] = [model for model in json_cfg[\"models\"] if model.get(\"include_model\", True)]\n",
    "for model_dict in json_cfg[\"models\"]:\n",
    "    del model_dict[\"include_model\"]\n",
    "    model = model_dict[\"model\"].strip()\n",
    "    model_url = \"\"\n",
    "    if model != \"\":\n",
    "      if model.startswith('https') and model.endswith('.gguf'):\n",
    "        model_url = model\n",
    "      else:\n",
    "        model_name = model.split('/')[-1].lower().rstrip(\"-gguf\") + f\".{quant_method}.gguf\"\n",
    "        model_url = f\"https://huggingface.co/{model}/resolve/main/{model_name}\"\n",
    "    download_cmd = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {model_url} -d {models_dir} -o {model_name}\"\n",
    "    print(download_cmd)\n",
    "    !$download_cmd\n",
    "    model_dict[\"model\"]: str = f\"{models_dir}/{model_name}\"\n",
    "    model_dict[\"n_gpu_layers\"]: int = n_gpu_layers\n",
    "    model_dict[\"offload_kqv\"] = offload_kqv\n",
    "    model_dict[\"n_threads\"]: int = n_threads\n",
    "    model_dict[\"n_batch\"]: int = n_batch\n",
    "    model_dict[\"n_ctx\"]: int = n_ctx\n",
    "    model_dict[\"cache\"] = cache\n",
    "\n",
    "port = json_cfg[\"port\"]\n",
    "with open('CONFIG_FILE.json', 'w') as json_file:\n",
    "    json.dump(json_cfg, json_file)\n",
    "\n",
    "!python3 -m llama_cpp.server --config_file CONFIG_FILE.json > server.log 2>&1 &\n",
    "\n",
    "# Start ngrok tunnel\n",
    "# add secret variable NGROK_AUTHTOKEN with Authtoken value from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "!ngrok config add-authtoken {ngrok_auth_token}\n",
    "print(f\"Check API: https://{ngrok_domain}/docs\")\n",
    "!ngrok http --domain={ngrok_domain} {port}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMd/BbIoY7Mr2J+rD54WhFv",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
