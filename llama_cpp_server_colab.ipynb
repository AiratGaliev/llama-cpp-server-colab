{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AiratGaliev/llama-cpp-server-colab/blob/main/llama-cpp-server-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC4zIEuXlTqg",
    "outputId": "bea6ab6d-53cc-4306-d0df-6f685766a785"
   },
   "outputs": [],
   "source": [
    "#@title Start the llama-cpp-server\n",
    "\n",
    "#@markdown If unsure about the branch, write \"main\" or leave it blank.\n",
    "%cd /content\n",
    "!apt-get -y install -qq aria2\n",
    "!npm install -g localtunnel\n",
    "!export CUDACXX=/usr/local/cuda/bin/nvcc\n",
    "!export CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n",
    "!export FORCE_CMAKE=1\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install llama-cpp-python[server]==0.2.38 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "\n",
    "# Parameters\n",
    "model_url = \"TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF\" #@param {type:\"string\"}\n",
    "model_alias = \"dolphin-2.6-mistral-7B-dpo-laser\" #@param {type:\"string\"}\n",
    "chat_format = \"chatml\" #@param {type:\"string\"}\n",
    "n_gpu_layers = 33 #@param {type:\"integer\"}\n",
    "offload_kqv = True, #@param {type:\"boolean\"}\n",
    "n_threads = 12, #@param {type:\"integer\"}\n",
    "n_batch = 512, #@param {type:\"integer\"}\n",
    "n_ctx = 5120, #@param {type:\"integer\"}\n",
    "cache = False #@param {type:\"string\"}\n",
    "\n",
    "model_url = model_url.strip()\n",
    "model = \"\"\n",
    "if model_url != \"\":\n",
    "    if not model_url.startswith('http'):\n",
    "        model_url = f\"https://huggingface.co/{model_url}/blob/main/{model_alias}.Q8_0.gguf\"\n",
    "\n",
    "# Download the model\n",
    "models_dir = f\"/content/models/\"\n",
    "download_cmd = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {model_url} -d {models_dir} -o {model_alias}.Q8_0.gguf\"\n",
    "print(download_cmd)\n",
    "!$download_cmd\n",
    "\n",
    "# Start the server\n",
    "model = f\"{models_dir}{model_alias}.Q8_0.gguf\"\n",
    "command_line_flags = \"\"\n",
    "for param in ['model','model_alias','chat_format', 'n_gpu_layers', 'offload_kqv', 'n_threads', 'n_batch', 'n_ctx', 'cache']:\n",
    "    command_line_flags += f\" --{param} {param}\"\n",
    "\n",
    "cmd = f\"python -m llama_cpp.server --port 1234 {command_line_flags}\"\n",
    "print(cmd)\n",
    "!$cmd"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMd/BbIoY7Mr2J+rD54WhFv",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
