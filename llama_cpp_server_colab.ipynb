{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<a href=\"https://colab.research.google.com/github/AiratGaliev/llama-cpp-server-colab/blob/main/llama_cpp_server_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@title Start llama-cpp-server\n",
    "\n",
    "#@markdown If unsure about the branch, write \"main\" or leave it blank.\n",
    "%cd /content\n",
    "!apt-get -y install -qq aria2\n",
    "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | tee /etc/apt/sources.list.d/ngrok.list\n",
    "!apt update\n",
    "!apt list --upgradable\n",
    "!apt dist-upgrade\n",
    "!apt install ngrok\n",
    "!CUDACXX=/usr/local/cuda/bin/nvcc CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python[server]==0.2.38\n",
    "\n",
    "json_cfg = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 1234,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model\": \"https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF\",\n",
    "            \"model_alias\": \"dolphin\",\n",
    "            \"chat_format\": \"chatml\"\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\",\n",
    "            \"model_alias\": \"openchat\",\n",
    "            \"chat_format\": \"openchat\"\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"https://huggingface.co/TheBloke/xDAN-L1-Chat-RL-v1-GGUF\",\n",
    "            \"model_alias\": \"xdan\",\n",
    "            \"chat_format\": \"chatml\"\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-16k-GGUF\",\n",
    "            \"model_alias\": \"openhermes\",\n",
    "            \"chat_format\": \"chatml\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "ngrok_auth_token = \"\" #@param {type:\"string\"}\n",
    "model_url = \"TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF\" #@param {type:\"string\"}\n",
    "model_alias = \"dolphin-2.6-mistral-7b-dpo-laser\" #@param {type:\"string\"}\n",
    "chat_format = \"chatml\" #@param {type:\"string\"}\n",
    "n_gpu_layers = 50 #@param {type:\"integer\"}\n",
    "offload_kqv = True #@param {type:\"boolean\"}\n",
    "n_threads = 12 #@param {type:\"integer\"}\n",
    "n_batch = 2048 #@param {type:\"integer\"}\n",
    "n_ctx = 16384 #@param {type:\"integer\"}\n",
    "cache = False #@param {type:\"boolean\"}\n",
    "quant_method = \"Q8_0\" # @param [\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\"]\n",
    "\n",
    "models_dir = \"/content/models\"\n",
    "\n",
    "for models in json_cfg[\"models\"]:\n",
    "    models[\"model\"]: str = models_dir + models[\"model\"].split('/')[-1].lower().rstrip(\n",
    "        \"-gguf\") + f\".{quant_method}.gguf\"\n",
    "    models[\"n_gpu_layers\"]: int = n_gpu_layers\n",
    "    models[\"offload_kqv\"]: bool = offload_kqv\n",
    "    models[\"n_threads\"]: int = n_threads\n",
    "    models[\"n_batch\"]: int = n_batch\n",
    "    models[\"n_ctx\"]: int = n_ctx\n",
    "    models[\"cache\"]: bool = cache\n",
    "\n",
    "model_url = model_url.strip()\n",
    "model = \"\"\n",
    "if model_url != \"\":\n",
    "    if not model_url.startswith('http'):\n",
    "        model_url = f\"https://huggingface.co/{model_url}/resolve/main/{model_alias}.Q8_0.gguf\"\n",
    "\n",
    "# Download models\n",
    "download_cmd = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {model_url} -d {models_dir} -o {model_alias}.Q8_0.gguf\"\n",
    "print(download_cmd)\n",
    "!$download_cmd\n",
    "\n",
    "# Start server\n",
    "model = f\"{models_dir}/{model_alias}.Q8_0.gguf\"\n",
    "host = json_cfg[\"host\"]\n",
    "port = json_cfg[\"port\"]\n",
    "\n",
    "!python3 -m llama_cpp.server --host {host} --port {port} --model /content/models/dolphin-2.6-mistral-7b-dpo-laser.Q8_0.gguf --model_alias {model_alias} --chat_format {chat_format} --n_gpu_layers {n_gpu_layers} --offload_kqv {offload_kqv} --n_threads {n_threads} --n_batch {n_batch} --n_ctx {n_ctx} --cache {cache} > server.log 2>&1 &\n",
    "\n",
    "# Start ngrok tunnel\n",
    "# add secret variable NGROK_AUTHTOKEN with Authtoken value from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "!ngrok config add-authtoken {ngrok_auth_token}\n",
    "!ngrok http --domain=wasp-immortal-factually.ngrok-free.app {port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pkill ngrok"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMd/BbIoY7Mr2J+rD54WhFv",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
